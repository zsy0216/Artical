# 1、Requests库入门

## Requests安装

用管理员身份打开命令提示符：

```shell
pip install requests
```

测试：打开IDLE：

```python
>>> import requests
>>> r = requests.get("http://www.baidu.com")
>>> r.status_code
200
>>> r.encoding = 'utf-8' #修改默认编码
>>> r,text		#打印网页内容
```

## HTTP协议

超文本传输协议,Hypertext Transfer Protocol.

HTTP是一个基于“请求与响应”模式的、无状态的应用层协议。

HTTP协议采用URL作为定位网络资源的标识。

### URL格式

`http://host[:port][path]`

host:合法的Internet主机域名或IP地址

port：端口号，缺省端口为80

path：请求资源的路径

### 操作

| 方法   | 说明                                                      |
| ------ | --------------------------------------------------------- |
| GET    | 请求获取URL位置的资源                                     |
| HEAD   | 请求获取URl位置资源的响应消息报告，即获得该资源的头部信息 |
| POST   | 请求向URL位置的资源后附加新的数据                         |
| PUT    | 请求向URL位置存储一个资源，覆盖原URL位置的资源            |
| PATCH  | 请求局部更新URL位置的资源，即改变该处资源的部分内容       |
| DELETE | 请求删除URL位置存储的资源                                 |



## Requests主要方法

| 方法               | 说明                                           |
| ------------------ | ---------------------------------------------- |
| requests.request() | 构造一个请求，支撑以下各方法的基础方法         |
| requests.get()     | 获取HTML网页的主要方法，对应于HTTP的GET        |
| requests.head()    | 获取HTML网页头信息的方法，对应于HTTP的HEAD     |
| requests.post()    | 向HTML网页提交POST请求的方法，对应于HTTP的POST |
| requests.put()     | 向HTML网页提交PUT请求的方法，对应于HTTP的PUT   |
| requests.patch()   | 向HTML网页提交局部修改请求，对应于HTTP的PATCH  |
| requests.delete()  | 向HTML网页提交删除请求，对应于HTTP的DELETE     |

主要方法为request方法，其他方法都是在此方法基础上封装而来以便使用。

### request()方法

```python
requests.request(method,url,**kwargs)
#method:请求方式，对应get/put/post等7种
#url：拟获取页面的url链接
#**kwargs：控制访问的参数，共13个
```

**kwargs：控制访问的参数，均为可选项

### get()方法

```python
r  = requests.get(url)
完整方法：
requests.get(url,params=None,**kwargs)
	url:拟获取页面的url链接
	params:url中的额外参数，字典或字节流格式，可选
	**kwargs:12个控制访问的参数，可选
```

get()方法：

构造一个向服务器请求资源的Request对象

返回一个包含服务器资源的Response对象

#### Response对象

| 属性                | 说明                                             |
| ------------------- | ------------------------------------------------ |
| r.status_code       | HTTP请求的返回状态，200表示连接成功，404表示失败 |
| r.text              | HTTP响应内容的字符串形式，即：url对应的页面内容  |
| r.encoding          | 从HTTP header中猜测的响应内容编码方式            |
| r.apparent_encoding | 从内容中分析出的响应内容编码方式（备选编码方式） |
| r.content           | HTTP响应内容的二进制形式                         |

### head()方法

```pyhton
r = requests.head('http://httpbin.org/get')
r.headers
```

获取网络资源的概要信息

### post()方法

向服务器提交新增数据

```python
payload = {'key1':'value1','key2':'value2'} #新建一个字典
#向URL POST一个字典，自动编码为form（表单）
r = requests.post('http://httpbin.org/post',data = payload)
#向URL POST一个字符串，自动编码为data
r = requests.post('http://httpbin.org/post',data = 'ABC') 
print(r.text)     
```

### put()方法

同post，只不过会把原来的内容覆盖掉。

### patch()方法

### delete()方法

## Requests库的异常

| 异常                      | 说明                                        |
| :------------------------ | ------------------------------------------- |
| requests.ConnectionError  | 网络连接错误异常，如DNS查询失败、拒绝连接等 |
| requests.HTTPError        | HTTP错误异常                                |
| requests.URLRequired      | URL缺失异常                                 |
| requests.TooManyRedirects | 超过最大 重定向次数，产生重定向异常         |
| requests.ConnectTimeout   | 连接远程服务器超时异常                      |
| requests.Timeout          | 请求URL超时，产生超时异常                   |

| 异常方法           | 说明                                  |
| ------------------ | ------------------------------------- |
| r.raise_for_status | 如果不是200产生异常requests.HTTPError |

## 爬取网页的通用代码框架

```python
import requests

def getHTMLText(url):
    try:
        r = requests.get(url,timeout=30)
        r.raise_for_status() #如果不是200，引发HTTPError异常
        r.recoding = r.apparent_encoding
        return r.text
    except:
        return "产生异常"
if __name__ == "__main__":
    url = "http://www.baidu.com"
    print(getHTMLText(url))
```

## 实例

### 向百度提交关键词

```python
import requests

# 向搜索引擎进行关键词提交
url = "http://www.baidu.com"
try:
    kv = {'wd':'python'}
    r = requests.get(url,params =kv)
    print(r.request.url)
    r.raise_for_status()
    print(len(r.text))
except:
    print("产生异常")
```

### 获取网络图片及存储

```python
import requests
import os
url = "http://image.ngchina.com.cn/2019/0423/20190423024928618.jpg"
root = "D://2345//Temp//"
path = root + url.split('/')[-1]
try:
    if not os.path.exists(root):
        os.mkdir(root)
    if not os.path.exists(path):
        r = requests.get(url)
        with open(path,'wb') as f:
            f.write(r.content)  #r.content返回二进制内容
            f.close()
            print("文件保存成功")
    else:
        print("文件已存在")
except:
    print("爬取失败")
```

# 2、信息提取之Beautiful Soup库入门

## Beautiful Soup库安装

```shell
pip install beautifulsoup4
```

测试：

```python
import requests
r = requests.get("http://python123.io/ws/demo.html")
demo = r.text
form bs4 import BeautifulSoup #从bs4中引入BeautifulSoup类
soup = BeautifulSoup(demo, "html.parser")
```

Beautiful Soup库是解析、遍历、维护“标签树”的功能库

## Beautiful Soup库的基本元素

### Beautiful Soup库的引用

Beautiful Soup库，也叫beautifulsoup4或bs4.

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(demo,"html.parser")
```

### Beautiful Soup类的基本元素

| 基本元素        | 说明                                                     |
| --------------- | -------------------------------------------------------- |
| Tag             | 标签，最基本的信息组织单元，分别用<>和</>标明开头和结尾  |
| Name            | 标签的名字，<p>...</p>的名字是'p'，格式：<tag>.name      |
| Attributes      | 标签的属性，字典形式组织，格式：<tag>.attrs              |
| NavigableString | 标签内非属性字符串，<>...</>中字符串，格式：<tag>.string |
| Comment         | 标签内字符串的注释部分，一种特殊的Comment类型            |

## 基于bs4库的HTML内容遍历方法

### 下行遍历

| 属性                | 说明                                                    |
| ------------------- | ------------------------------------------------------- |
| .contents(列表类型) | 子节点的列表，将<tag>所有儿子节点存入列表               |
| .children           | 子节点的迭代类型，与.contents类似，用于循环遍历儿子节点 |
| .descendants        | 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历      |

```python
#遍历儿子节点
for child in soup.body.children
	print(child)
#遍历子孙节点
for child in soup.body.descendants
	print(child)
```

### 上行遍历

| 属性     | 说明                                         |
| -------- | -------------------------------------------- |
| .parent  | 节点的父亲标签                               |
| .parents | 节点先辈标签的迭代类型，用于循环遍历先辈节点 |

```python
soup = BeautifulSoup(demo,"html.parser")
for parent in soup.a.parents:
    if parent is None:
        print(parent)
    else:
        print(parent.name)
#输出结果
#p
#body
#html
#[document]
```

### 平行遍历

平行遍历发生在同一个父节点下的各节点间。

下一个获取的可能是字符串类型，不一定是下一个节点。

| 属性               | 说明                                                 |
| ------------------ | ---------------------------------------------------- |
| .next_sibling      | 返回按照HTML文本顺序的下一个平行节点标签             |
| .previous_sibling  | 返回按照HTML文本顺序的上一个平行节点标签             |
| .next_siblings     | 迭代类型，返回按照HTML文本顺序的后续所有平行节点标签 |
| .previous_siblings | 迭代类型，返回按照HTML文本顺序的前续所有平行节点标签 |

```python
#遍历后续节点
for sibling in soup.a.next_siblings
	print(sibling)
#遍历前续节点
for sibling in soup.a.previous_siblings
	print(sibling)
```

## 基于bs4库的HTML格式化和编码

### 格式化方法：.prettify()

```python
soup = BeautifulSoup(demo,"html.parser")
print(soup.a.prettify())
```

### 编码：默认utf-8

```python
soup = BeautifulSoup("<p>中文</p>","html.parser")
soup.p.string
#'中文'
print(soup.p.prettify())
#<p>
#  中文
#</p>
```

# 3.信息组织与提取

## 信息标记的三种形式

标记后的信息可形成信息组织结构，增加了信息的维度；

标记后的信息可用于通信、存储和展示；

标记的结构和信息一样具有重要价值；

标记后的信息有利于程序的理解和运用。

### XML: eXtensible Matkup Language

最早的通用信息标记语言，可扩展性好，但繁琐。

用于Internet上的信息交互和传递。

```xml
<name>...</name>
<name/>
<!--  -->
```

### JSON:  JavaScript Object Notation

信息有类型，适合程序处理(js)，较XML简洁。

用于移动应用云端和节点的信息通信，无注释。

```json
#有类型的键值对表示信息的标记形式
"key":"value"
"key":["value1","value2"]
"key":{"subkey":"subvalue"}
```

### YAMl: YAML Ain't Markup Language

信息无类型，文本信息比例最高，可读性好。

用于各类系统的配置文件，有注释易读。

```yaml
#无类型的键值对表示信息的标记形式
key : "value"
key : #comment
-value1
-value2
key :
	subkey : subvalue
```

## 信息提取的一般方法

### 方法一：完整解析信息的标记形式，再提取关键信息。

XML JSON YAML

需要标记解析器，例如bs4库的标签树遍历。

优点：信息解析准确

缺点：提取过程繁琐，过程慢

### 方法二：无视标记形式，直接搜索关键信息

搜索

对信息的文本查找函数即可。

优点：提取过程简洁，速度较快

缺点：提取过程准确性与信息内容相关

### 融合方法：结合形式解析与搜索方法,提取关键信息

XML JSON YAML  搜索

需要标记解析器及文本查找函数。

### 实例：提取HTML中所有URL链接

思路：	1. 搜索到所有<a>标签

​		2.解析<a>标签格式，提取href后的链接内容

```python
form bs4 import BeautifulSoup
soup = BeautifulSoup(demo,"html.parser")
for link in soup.find_all('a'):
	print(link.get('href'))
    
```

## 基于bs4库的HTML内容查找方法

| 方法                                              | 说明                             |
| ------------------------------------------------- | -------------------------------- |
| <>.find_all(name,attrs,recursive,string,**kwargs) | 返回一个列表类型，存储查找的结果 |

简写形式：<tag>(..) 等价于 <tag>.find_all(..)

```python
#name:对标签名称的检索字符串
soup.find_all('a')
soup.find_all(['a', 'b'])
soup.find_all(True) #返回soup的所有标签信息
for tag in soup.find_all(True):
    print(tag.name) #html head title body p b p a a
#输出所有b开头的标签，包括b和body    
#引入正则表达式库
import re
for tag in soup.find_all(re.compile('b')):
    print(tag.name) #body b

#attrs:对标签属性值的检索字符串，可标注属性检索
soup.find_all('p', 'course')
soup.find_all(id='link1')
import re 
soup.find_all(id=re.compile('link'))

#recursive:是否对子孙全部检索，默认为True
soup.find_all('p', recursive = False)

#string:<>...</>字符串区域的检索字符串
soup.find_all(string = "Basic Python")
import re
soup.find_all(string = re.compile('Python'))
#简写形式：soup(..) = soup.find_all(..)
```

拓展方法：参数同.find_all()

| 方法                        | 说明                                     |
| --------------------------- | ---------------------------------------- |
| <>.find()                   | 搜索且只返回一个结果，字符串类型         |
| <>.find_parents()           | 在先辈节点中搜索，返回列表类型           |
| <>.find_parent()            | 在先辈节点中返回一个结果，字符串类型     |
| <>.find_next_siblings()     | 在后续平行节点中搜索，返回列表类型       |
| <>.find_next_sibling()      | 在后续平行节点中返回一个结果，字符串类型 |
| <>.find_previous_siblings() | 在前续平行节点中搜索，返回列表类型       |
| <>.find_previous_sibling()  | 在前续平行节点中返回一个结果，字符串类型 |

# 4.信息提取实例

## 中国大学排名定向爬虫

功能描述：

​	输入：大学排名URL链接

​	输出：大学排名信息的屏幕输出（排名，大学名称，总分）

​	技术路线：requests-bs4

​	定向爬虫：仅对输入URL进行爬取，不拓展爬取

程序的结构设计：

​	步骤1：从网络上获取大学排名网页内容 

​			getHTMLText()

​	步骤2：提取网页内容中信息到合适的数据结构

​			fillUnivList()

​	步骤3：利用数据结构展示并输出结果

​			printUnivList()

## 初步代码编写

```python
import requests
from bs4 import BeautifulSoup
import bs4

def getHTMLText(url):
    try:
        r = requests.get(url, timeout= 30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return ""

def fillUnivList(ulist, html):
    soup = BeautifulSoup(html, "html.parser")
    for tr in soup.find('tbody').children:
        if isinstance(tr, bs4.element.Tag):
            tds = tr('td')
            ulist.append([tds[0].string, tds[1].string, tds[3].string])

def printUnivList(ulist, num):
    print("{:^10}\t{:^6}\t{:^10}".format("排名", "学校名称", "分数"))
    for i in range(num):
        u = ulist[i]
        print("{:^10}\t{:^6}\t{:^10}".format(u[0], u[1], u[2]))

def main():
    uinfo = []
    url = 'http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html'
    html = getHTMLText(url)
    fillUnivList(uinfo,html)
    printUnivList(uinfo,20) #20 univs
main()
```

## 中文输出对齐问题

当输出中文的宽度不够时，系统会采用西文字符填充，导致对齐出现问题。

可以使用中文空格chr(12288)填充解决。

`<填充>`：用于填充的单个字符

`<对齐>`：<左对齐    >右对齐	    ^居中对齐

`<宽度>`：槽的设定输出宽度

`,`：数字的千位分隔符适用于整数和浮点数

`<精度>`：浮点数小数部分的精度或字符串的最大输出长度

`<类型>`：整数类型b,c,d,o,x,X浮点数类型e,E,f,%

## 代码优化

```python
import requests
from bs4 import BeautifulSoup
import bs4

def getHTMLText(url):
    try:
        r = requests.get(url, timeout= 30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return ""

def fillUnivList(ulist, html):
    soup = BeautifulSoup(html, "html.parser")
    for tr in soup.find('tbody').children:
        if isinstance(tr, bs4.element.Tag):
            tds = tr('td')
            ulist.append([tds[0].string, tds[1].string, tds[3].string])

def printUnivList(ulist, num):
    tplt = "{0:^10}\t{1:{3}^10}\t{2:^10}"
    print(tplt.format("排名", "学校名称", "分数",chr(12288)))
    for i in range(num):
        u = ulist[i]
        print(tplt.format(u[0], u[1], u[2],chr(12288)))

def main():
    uinfo = []
    url = 'http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html'
    html = getHTMLText(url)
    fillUnivList(uinfo,html)
    printUnivList(uinfo,20) #20 univs
main()
```





来源：中国大学MOOC-北京理工大学-嵩天